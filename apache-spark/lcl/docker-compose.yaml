# Before `docker compose up`, you build the base images:
# >> docker build -f ./dockerfiles/apache-spark-base.dockerfile -t apache-spark-base .
# >> docker build -f ./dockerfiles/apache-spark.dockerfile -t apache-spark .
# >> docker build -f ./dockerfiles/apache-spark-jupyterlab.dockerfile -t pyspark-jupyterlab .
#
# >> docker compose up --scale spark-worker=3

version: '3'

volumes:
  workspace:

networks:
  spark-cluster:

services:

  pyspark-jupyterlab:
    image: pyspark-jupyterlab:latest
    container_name: pyspark-jupyterlab
    ports:
      - 8888:8888
      - 4040:4040
    networks:
      - spark-cluster
    volumes:
      - ./workspace:/opt/workspace
  
  spark-master:
    container_name: spark-master
    image: apache-spark:latest
    restart: unless-stopped
    ports:
      - 8080:8080
      - 7077:7077
    networks:
      - spark-cluster
    volumes:
      - ./workspace:/opt/workspace
    command: bin/spark-class org.apache.spark.deploy.master.Master --ip spark-master --port 7077 --webui-port 8080
  
  spark-worker:
    image: apache-spark:latest
    restart: unless-stopped
    depends_on:
      - spark-master
    environment:
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 1024mb
    networks:
      - spark-cluster
    volumes:
      - ./workspace:/opt/workspace
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077